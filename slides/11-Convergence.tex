%!TEX root = slides.tex

\section{Convergence Diagnostics}

\subsection{MCMC Convergence}
\begin{frame}{Markov Chain Convergence}
    MCMC has an interesting property that it will
    \textbf{asymptotically converge to the target distribution}\footnote{
        this property is not present on neural networks.}.
    \vfill
    That means, if we have all the time in the world, it is guaranteed,
    irrelevant of the target distribution posterior geometry,
    \textbf{MCMC will give you the right answer}.
    \vfill
    However, we don't have all the time in the world
    Different MCMC algorithms, like HMC and NUTS,
    can reduce the sampling (and warmup) time necessary for convergence to the target distribution.
\end{frame}

\begin{frame}{Can We Prove Convergence?}
    \begin{vfilleditems}
        \item In the ideal scenario, the NUTS sampler converges to the true posterior and doesn't miss on any mode.
        \item Unfortunately, this is not easy to prove in general.
        \item All the convergence diagnostics are only tests for symptoms of lack of convergence.
        \item In other words if all the diagnostics look normal, then we can't prove that the sampler didn't converge.
        \item But we also can't prove that the sampler actually converged.
    \end{vfilleditems}
\end{frame}

\begin{frame}{Signs of Lack of Convergence}
    \begin{vfilleditems}
        \item Some signs of lack of convergence are:
            \begin{vfilleditems}
                \item Any of the moments (e.g. the mean or standard deviation) is changing with time. This is diagnosed using stationarity tests by comparing different parts of a single chain to each other.
                \item Any of the moments is sensitive to the initial parameter values. This is diagnosed using multiple chains by comparing their summary statistics to each other.
            \end{vfilleditems}
        \item While high auto-correlation is not strictly a sign of lack of convergence, samplers with high auto-correlation will require many more samples to get to the same ESS as another sampler with low auto-correlation. So a low auto-correlation is usually more desirable.
    \end{vfilleditems}
\end{frame}

\subsection{Diagnostics}
\begin{frame}{Convergence Metrics}
    There are a few metrics and diagnostics usually used to assess and diagnose the Markov chains:
    \begin{vfilleditems}
        \item \textbf{Effective Sample Size} (ESS):
        an approximation of the ``number of independent samples'' generated by a Markov chain.
        \item $\widehat{R}$ (\textbf{Rhat}):
        potential scale reduction factor,
        a metric to measure if the Markov chain have mixed,
        and, potentially, converged.
    \end{vfilleditems}
\end{frame}

\begin{frame}{Effective Sample Size \parencite{gelman2013bayesian}}
    $$\widehat{n}_{\text{eff}} = \frac{mn}{1 + \sum_{t=1}^T \widehat{\rho}_t}$$
    Where:
    \begin{vfilleditems}
        \item $m$: number of Markov chains.
        \item $n$: total samples per Markov chain (discarding warmup).
        \item $\widehat{\rho}_t$: an autocorrelation estimate.
    \end{vfilleditems}
\end{frame}

\begin{frame}{Rhat \parencite{gelman2013bayesian}}
    $$\widehat{R} = \sqrt{\frac{\widehat{\text{var}}^+(\psi \mid y)}{W}}$$
    where $\widehat{\text{var}}^+(\psi \mid y)$ is the Markov chains' sample variance
    for a certain parameter $\psi$.
    We calculate it by using a weighted sum of the within-chain $W$
    and between-chain $B$ variances:
    $$\widehat{\text{var}}^+(\psi \mid y) = \frac{n-1}{n} W + \frac{1}{n} B$$
    \vfill
    Intuitively, the value is $1.0$ if all chains are totally convergent.
    As a heuristic, if $\widehat{R} > 1.1$,
    you need to worry because probably the chains have not converged adequate.
\end{frame}

% plots taken from script:
% slides/images/bad_chains_traceplot.tex
\begin{frame}{Traceplot -- Convergent Markov Chains}
    \begin{figure}
        \centering
        \resizebox{.4\linewidth}{!}{\input{images/good_chains_traceplot.tex}}
        \caption{A convergent Markov chains traceplot}
    \end{figure}
\end{frame}

\begin{frame}{Traceplot -- Divergent Markov Chains}
    \begin{figure}
        \centering
        \resizebox{.4\linewidth}{!}{\input{images/bad_chains_traceplot.tex}}
        \caption{A divergent Markov chains traceplot}
    \end{figure}
\end{frame}

% \subsubsection{What To Do If the Markov Chains Do Not Converge?}
% \begin{frame}[fragile]{\texttt{Stan}'s Warning Messages\footnote{also see \href{https://mc-stan.org/misc/warnings.html}{\texttt{Stan}'s \textcolor{red}{warnings} guide}.}}
%     \begin{lstlisting}[basicstyle=\footnotesize\color{red}]
% Warning messages:
% 1: There were 275 divergent transitions after warmup. See
% http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
% to find out why this is a problem and how to eliminate them.
% 2: Examine the pairs() plot to diagnose sampling problems

% 3: The largest R-hat is 1.12, indicating chains have not mixed.
% Running the chains for more iterations may help. See
% http://mc-stan.org/misc/warnings.html#r-hat
% 4: Bulk Effective Samples Size (ESS) is too low, indicating posterior
% means and medians may be unreliable.
% Running the chains for more iterations may help. See
% http://mc-stan.org/misc/warnings.html#bulk-ess
% 5: Tail Effective Samples Size (ESS) is too low, indicating posterior
% variances and tail quantiles may be unreliable.
% Running the chains for more iterations may help. See
% http://mc-stan.org/misc/warnings.html#tail-ess
%   \end{lstlisting}
% \end{frame}
% % https://mc-stan.org/misc/warnings.html

% \begin{frame}[fragile]{\texttt{Turing.jl}'s Warning Messages}
%     \textbf{\texttt{Turing.jl} does not give warning messages!}
%     But you can check divergent transitions with $\texttt{summarize(chn; sections=[:internals])}$:
%     \vfill
%     %@model function funnel_cp()
%     %    y ~ Normal(0, 3)
%     %    x ~ Normal(0, exp(y/2))
%     % end
%     %chn = sample(funnel_cp(), NUTS(1_000, 0.8), MCMCThreads(), 1_000, 4)
%     \begin{lstlisting}[basicstyle=\footnotesize]
% Summary Statistics
%       parameters     mean      std  naive_se     mcse      ess     rhat  ess_per_sec
%           Symbol  Float64  Float64   Float64  Float64  Float64  Float64  Float64

%               lp  -3.9649   1.7887   0.0200   0.1062  179.1235  1.0224   6.4133
%          n_steps   9.1275  11.1065   0.1242   0.7899   38.3507  1.3012   1.3731
%  acceptance_rate   0.5944   0.4219   0.0047   0.0322   40.5016  1.2173   1.4501
%       tree_depth   2.2444   1.3428   0.0150   0.1049   32.8514  1.3544   1.1762
%  numerical_error   0.1975   0.3981   0.0045   0.0273   59.8853  1.1117   2.1441
%   \end{lstlisting}
% \end{frame}

\begin{frame}{Geweke Diagnostic}
\end{frame}

\begin{frame}{Hiedelberger Diagnostic}
\end{frame}

\begin{frame}{Raftery Diagnostic}
\end{frame}

\subsection{What To Do If the Markov Chains Do Not Converge?}
% \begin{frame}{What To Do If the Markov Chains Do Not Converge?}
%     \textbf{First}: before making any fine adjustments in the number of Markov chains
%     or the number of iterations per chain, etc.
%     Acknowledge that both \texttt{Stan}'s and \texttt{Turing.jl}'s NUTS sampler is
%     \textbf{very efficient and effective in exploring the
%         most crazy and diverse target posterior densities}.
%     \vfill
%     And the standard settings, \textbf{2,000 iterations and 4 chains},
%     works perfectly for 99\% of the time.
% \end{frame}

\begin{frame}{What To Do If the Markov Chains Do Not Converge?}
    \vfill
    \begin{quotation}
        When you have computational problems,
        often there’s a problem with your model.
    \end{quotation}
    \vfill \vfill
    \textcite{gelmanFolkTheoremStatistical2008} (Folk Theorem)
\end{frame}

% \begin{frame}{What To Do If the Markov Chains Do Not Converge?}
%     If you are experiencing convergence issues,
%     \textbf{and you've discarded that something is wrong with you model},
%     here is a few steps to try\footnote{
%         besides that,
%         maybe should be worth to do a QR decomposition in the data matrix $\mathbf{X}$,
%         thus having an orthogonal basis (non-correlated) for the sampler to explore.
%         This makes the target distribution's geometry much more friendlier,
%         in the topological/geometrical sense,
%         for the MCMC sampler explore.
%         Check the \hyperlink{appendixqr}{backup slides}.}.
%     Here listed in increasing complexity:
%     \begin{vfilleditems}
%         \item[1.] \textbf{Increase the number of iterations and chains}:
%         try first increasing the number of iterations,
%         then try increasing the number of chains.
%         (remember the default is 2,000 iterations and 4 chains).
%     \end{vfilleditems}
% \end{frame}

% \begin{frame}{What To Do If the Markov Chains Do Not Converge?}
%     \begin{vfilleditems}
%         \item[2.] \textbf{Change the HMC's warmup adaptation routine}:
%         make the HMC sampler to be more conservative in the proposals.
%         This can be changed by increasing the hyperparameter
%         \textbf{target acceptance rate of Metropolis proposals}\footnote{
%             \texttt{Stan}'s default is \texttt{0.8} and
%             \texttt{Turing.jl}'s default is \texttt{0.65}.}.
%         The maximum value is $1.0$ (not recommended).
%         Então qualquer valor entre $0.8$ e $1.0$ o torna mais conservador.
%         \item[3.] \textbf{Model reparameterization}:
%         there are two approaches.
%         Centered parameterization (CP) and non-centered parameterization (NCP).
%     \end{vfilleditems}
% \end{frame}

% \begin{frame}{What To Do If the Markov Chains Do Not Converge?}
%     \begin{vfilleditems}
%         \item[4.] \textbf{Collect more data}:
%         sometimes the model is too complex and we need a higher sample size for stable estimates.
%         \item[5.] \textbf{Rethink the model}:
%         convergence issues with an adequate sample size might be due to
%         incompatibility between priors and likelihood function(s).
%         In this case you need to rethink the whole data generating process
%         underlying the model, in which the model assumptions stems from.
%     \end{vfilleditems}
% \end{frame}

\begin{frame}{What To Do If the Markov Chains Do Not Converge?}
    \begin{vfilleditems}
        \item Dynamics-based models with complicated stiff differential equations often suffer from sensitivity to parameter values in 2 ways:
            \begin{vfilleditems}
                \item First, small changes in the parameter values may lead to extremely different dynamics and wrong predictions thus leading to rejections.
                \item And second, changes in the parameter values may make the differential equation highly stiff thus slowing down convergence or even causing divergence of the solver.
            \end{vfilleditems}
        \item In other words, MCMC for some complicated models can often run slow and fail to give good ESS values at the end.
        \item In such cases, NUTS may not always be computationally feasible.
        \item But you can try any of the following remedies and workarounds to poke at the model.
    \end{vfilleditems}
\end{frame}

\begin{frame}{What To Do If the Markov Chains Do Not Converge?}
    \begin{vfilleditems}
        \item Lower the \lstinline{target_accept} option. This may alleviate the need for a small step size and a full tree exploration.
        \item Re-parameterize your model to have less parameter dependence.
        \item Fix some parameter values to known good values, e.g. values obtained by maximum-a-posteriori (MAP) optimization.
        \item Initialize the sampling from good parameter values.
        \item Use a stronger prior around suspected good parameter values.
        \item Simplify your model, e.g. using simpler dynamics.
        \item Try the marginal MCMC algorithm `MarginalMCMC` instead of the full joint MCMC algorithm \lstinline{BayesMCMC}.
    \end{vfilleditems}
\end{frame}

\begin{frame}{What To Do If the Markov Chains Do Not Converge?}
    \begin{vfilleditems}
        \item If you find the sampler regularly hitting the maximum tree depth of 10 in the initial exploration phase, it might make sense to decrease that initially to have quicker iterations when in the exploration phase of the study.
        \item This is effectively limiting the level of exploration in the sampling so it might make sense to use good initial values when doing this.
        \item However in the final phase of the study, it is best to make sure that the maximum tree depth is not reached by the sampler (increasing it if necessary).
        \item This might also slow down your sampling significantly so there can be a tradeoff here.
        \item It's also best to ensure that the sampler converges to the posterior when starting from multiple different random initial points using different chains.
    \end{vfilleditems}
\end{frame}

\subsection{Is Convergence Important?}

\begin{frame}{Is Convergence Important?}
    \begin{vfilleditems}
        \item Since we can't prove that the sampler explored the full posterior in general, is exploring the full posterior always absolutely necessary?
        \item That depends on what you want to do. If you are trying to answer questions about the parameters, e.g. estimating the probability that an effect is greater than or less than 0 for a go/no-go decision, then you need your sampler to sample from the true posterior.
        \item Of course, we cannot prove this in general anyways but you should generally follow all the best practices and you should not ignore signs of lack of convergence.
    \end{vfilleditems}
\end{frame}

\begin{frame}{Is Convergence Important?}
    \begin{vfilleditems}
        \item Some bad signs to watch out for if you want to sample from the true posterior are:
            \begin{vfilleditems}
                \item Non-stationarity of the samples' distribution
                \item Dependence of the samples' distribution on the initial parameters after the adaptation steps
                \item High auto-correlation in the samples after the adaptation steps
                \item Too many rejections and ODE solver divergences
                \item Low ESS values relative to the number of samples 
                \item Extremely small step sizes and hitting the maximum tree depth often
            \end{vfilleditems}
    \end{vfilleditems}
\end{frame}
\begin{frame}{Is Convergence Important?}
    \begin{vfilleditems}
        \item On the other hand, if your goal is not to answer questions about the parameters but only to make predictions using the posterior predictive distribution as an ensemble of predictions, then sampling from the true posterior may not be strictly necessary in this case.
        \item If the posterior predictive distribution gives enough accuracy and uncertainty in the predictions to reflect the uncertainty in the unseen data, then that may suffice and we can live with some imperfections in the sampling.
    \end{vfilleditems}
\end{frame}

\begin{frame}{Is Convergence Important?}
    \begin{vfilleditems}
        \item Some imperfections in the sampling include:
            \begin{vfilleditems}
                \item Having to initialize the sampler from a mode to get the sampler to work
                \item Using a low maximum tree depth and allowing the maximum to be reached
                \item Using a high target acceptance ratio to decrease exploration and sample around a mode
                \item High auto-correlation in the samples even after the adaptation steps and low relative ESS
                \item Using a few adaptation steps `nadapts`
            \end{vfilleditems}
    \end{vfilleditems}
\end{frame}

\begin{frame}{Is Convergence Important?}
    \begin{vfilleditems}
        \item If doing any or all of the above resulted in fast sampling that gives a good enough posterior predictive distribution but potentially bad posterior exploration and if predictions are what you care the most about then perhaps you don't need to sample from the true posterior in your use case.
        \item Such an imperfect solution is often satisfactory in the context of Bayesian neural networks for example where parameters are generally meaningless and we only care about predictions.
        \item Of course, we don't advocate for doing this in general since this goes against the best practices of MCMC but it's an option you have.
    \end{vfilleditems}
\end{frame}
