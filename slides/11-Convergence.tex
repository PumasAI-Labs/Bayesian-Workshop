\subsection{Markov Chain Convergence}
\begin{frame}{Markov Chain Convergence}
    MCMC has an interesting property that it will
    \textbf{asymptotically converge to the target distribution}\footnote{
        this property is not present on neural networks.}.
    \vfill
    That means, if we have all the time in the world, it is guaranteed,
    irrelevant of the target distribution posterior geometry,
    \textbf{MCMC will give you the right answer}.
    \vfill
    However, we don't have all the time in the world
    Different MCMC algorithms, like HMC and NUTS,
    can reduce the sampling (and warmup) time necessary for convergence to the target distribution.
\end{frame}

\subsubsection{Convergence Metrics}
\begin{frame}{Convergence Metrics}
    We have some options on how to measure if the Markov chains converged to the target distribution,
    i.e. if they are ``reliable'':
    \begin{vfilleditems}
        \item \textbf{Effective Sample Size} (ESS):
        an approximation of the ``number of independent samples'' generated by a Markov chain.
        \item $\widehat{R}$ (\textbf{Rhat}):
        potential scale reduction factor,
        a metric to measure if the Markov chain have mixed,
        and, potentially, converged.
    \end{vfilleditems}
\end{frame}

\begin{frame}{Convergence Metrics -- Effective Sample Size \parencite{gelman2013bayesian}}
    $$\widehat{n}_{\text{eff}} = \frac{mn}{1 + \sum_{t=1}^T \widehat{\rho}_t}$$
    Where:
    \begin{vfilleditems}
        \item $m$: number of Markov chains.
        \item $n$: total samples per Markov chain (discarding warmup).
        \item $\widehat{\rho}_t$: an autocorrelation estimate.
    \end{vfilleditems}
\end{frame}

\begin{frame}{Convergence Metrics -- Rhat \parencite{gelman2013bayesian}}
    $$\widehat{R} = \sqrt{\frac{\widehat{\text{var}}^+(\psi \mid y)}{W}}$$
    where $\widehat{\text{var}}^+(\psi \mid y)$ is the Markov chains' sample variance
    for a certain parameter $\psi$.
    We calculate it by using a weighted sum of the within-chain $W$
    and between-chain $B$ variances:
    $$\widehat{\text{var}}^+(\psi \mid y) = \frac{n-1}{n} W + \frac{1}{n} B$$
    \vfill
    Intuitively, the value is $1.0$ if all chains are totally convergent.
    As a heuristic, if $\widehat{R} > 1.1$,
    you need to worry because probably the chains have not converged adequate.
\end{frame}

\subsubsection{Convergence Visualizations}
% plots taken from script:
% slides/images/bad_chains_traceplot.tex
\begin{frame}{Traceplot -- Convergent Markov Chains}
    \begin{figure}
        \centering
        \resizebox{.4\linewidth}{!}{\input{images/good_chains_traceplot.tex}}
        \caption{A convergent Markov chains traceplot}
    \end{figure}
\end{frame}

\begin{frame}{Traceplot -- Divergent Markov Chains}
    \begin{figure}
        \centering
        \resizebox{.4\linewidth}{!}{\input{images/bad_chains_traceplot.tex}}
        \caption{A divergent Markov chains traceplot}
    \end{figure}
\end{frame}

\subsubsection{What To Do If the Markov Chains Do Not Converge?}
\begin{frame}[fragile]{\texttt{Stan}'s Warning Messages\footnote{also see \href{https://mc-stan.org/misc/warnings.html}{\texttt{Stan}'s \textcolor{red}{warnings} guide}.}}
    \begin{lstlisting}[basicstyle=\footnotesize\color{red}]
Warning messages:
1: There were 275 divergent transitions after warmup. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
to find out why this is a problem and how to eliminate them.
2: Examine the pairs() plot to diagnose sampling problems

3: The largest R-hat is 1.12, indicating chains have not mixed.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#r-hat
4: Bulk Effective Samples Size (ESS) is too low, indicating posterior
means and medians may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#bulk-ess
5: Tail Effective Samples Size (ESS) is too low, indicating posterior
variances and tail quantiles may be unreliable.
Running the chains for more iterations may help. See
http://mc-stan.org/misc/warnings.html#tail-ess
  \end{lstlisting}
\end{frame}
% https://mc-stan.org/misc/warnings.html

\begin{frame}[fragile]{\texttt{Turing.jl}'s Warning Messages}
    \textbf{\texttt{Turing.jl} does not give warning messages!}
    But you can check divergent transitions with $\texttt{summarize(chn; sections=[:internals])}$:
    \vfill
    %@model function funnel_cp()
    %    y ~ Normal(0, 3)
    %    x ~ Normal(0, exp(y/2))
    % end
    %chn = sample(funnel_cp(), NUTS(1_000, 0.8), MCMCThreads(), 1_000, 4)
    \begin{lstlisting}[basicstyle=\footnotesize]
Summary Statistics
      parameters     mean      std  naive_se     mcse      ess     rhat  ess_per_sec
          Symbol  Float64  Float64   Float64  Float64  Float64  Float64  Float64

              lp  -3.9649   1.7887   0.0200   0.1062  179.1235  1.0224   6.4133
         n_steps   9.1275  11.1065   0.1242   0.7899   38.3507  1.3012   1.3731
 acceptance_rate   0.5944   0.4219   0.0047   0.0322   40.5016  1.2173   1.4501
      tree_depth   2.2444   1.3428   0.0150   0.1049   32.8514  1.3544   1.1762
 numerical_error   0.1975   0.3981   0.0045   0.0273   59.8853  1.1117   2.1441
  \end{lstlisting}
\end{frame}

\subsubsection{What To Do If the Markov Chains Do Not Converge?}
\begin{frame}{What To Do If the Markov Chains Do Not Converge?}
    \textbf{First}: before making any fine adjustments in the number of Markov chains
    or the number of iterations per chain, etc.
    Acknowledge that both \texttt{Stan}'s and \texttt{Turing.jl}'s NUTS sampler is
    \textbf{very efficient and effective in exploring the
        most crazy and diverse target posterior densities}.
    \vfill
    And the standard settings, \textbf{2,000 iterations and 4 chains},
    works perfectly for 99\% of the time.
\end{frame}

\begin{frame}{What To Do If the Markov Chains Do Not Converge?}
    \vfill
    \begin{quotation}
        When you have computational problems,
        often there’s a problem with your model.
    \end{quotation}
    \vfill \vfill
    \textcite{gelmanFolkTheoremStatistical2008} (Folk Theorem)
\end{frame}

\begin{frame}{What To Do If the Markov Chains Do Not Converge?}
    If you experiencing convergence issues,
    \textbf{and you've discarded that something is wrong with you model},
    here is a few steps to try\footnote{
        besides that,
        maybe should be worth to do a QR decomposition in the data matrix $\mathbf{X}$,
        thus having an orthogonal basis (non-correlated) for the sampler to explore.
        This makes the target distribution's geometry much more friendlier,
        in the topological/geometrical sense,
        for the MCMC sampler explore.
        Check the \hyperlink{appendixqr}{backup slides}.}.
    Here listed in increasing complexity:
    \begin{vfilleditems}
        \item[1.] \textbf{Increase the number of iterations and chains}:
        try first increasing the number of iterations,
        then try increasing the number of chains.
        (remember the default is 2,000 iterations and 4 chains).
    \end{vfilleditems}
\end{frame}

\begin{frame}{What To Do If the Markov Chains Do Not Converge?}
    \begin{vfilleditems}
        \item[2.] \textbf{Change the HMC's warmup adaptation routine}:
        make the HMC sampler to be more conservative in the proposals.
        This can be changed by increasing the hyperparameter
        \textbf{target acceptance rate of Metropolis proposals}\footnote{
            \texttt{Stan}'s default is \texttt{0.8} and
            \texttt{Turing.jl}'s default is \texttt{0.65}.}.
        The maximum value is $1.0$ (not recommended).
        Então qualquer valor entre $0.8$ e $1.0$ o torna mais conservador.
        \item[3.] \textbf{Model reparameterization}:
        there are two approaches.
        Centered parameterization (CP) and non-centered parameterization (NCP).
    \end{vfilleditems}
\end{frame}

\begin{frame}{What To Do If the Markov Chains Do Not Converge?}
    \begin{vfilleditems}
        \item[4.] \textbf{Collect more data}:
        sometimes the model is too complex and we need a higher sample size for stable estimates.
        \item[5.] \textbf{Rethink the model}:
        convergence issues with an adequate sample size might be due to
        incompatibility between priors and likelihood function(s).
        In this case you need to rethink the whole data generating process
        underlying the model, in which the model assumptions stems from.
    \end{vfilleditems}
\end{frame}